# Guide des Mod√®les IA pour Fricadelle

Ce guide vous aide √† choisir le meilleur mod√®le d'IA local pour analyser vos r√©sultats de scans de s√©curit√©.

## üéØ Mod√®les Recommand√©s

### 1. **Qwen2.5** (FORTEMENT RECOMMAND√â) üåü

#### Qwen2.5:14b
- **Qualit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent)
- **Vitesse**: ‚≠ê‚≠ê‚≠ê‚≠ê (Bon)
- **RAM requise**: ~10 GB
- **Pourquoi**: Meilleure compr√©hension contextuelle, excellente analyse de s√©curit√©
- **Installation**: `ollama pull qwen2.5:14b`

#### Qwen2.5:32b (Pour machines puissantes)
- **Qualit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent+)
- **Vitesse**: ‚≠ê‚≠ê‚≠ê (Moyen)
- **RAM requise**: ~20 GB
- **Pourquoi**: Meilleure qualit√© absolue, compr√©hension approfondie
- **Installation**: `ollama pull qwen2.5:32b`

**Avantages de Qwen2.5**:
- Excellente compr√©hension du contexte de s√©curit√©
- Meilleure identification des vraies vuln√©rabilit√©s
- Descriptions plus d√©taill√©es et professionnelles
- Moins de faux positifs
- Tr√®s bon en fran√ßais

### 2. **Llama 3.2** (D√âFAUT) ‚úÖ

#### Llama3.2:latest (3B)
- **Qualit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê (Tr√®s bon)
- **Vitesse**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent)
- **RAM requise**: ~4 GB
- **Pourquoi**: Bon √©quilibre qualit√©/vitesse, mod√®le par d√©faut
- **Installation**: `ollama pull llama3.2`

#### Llama3.1:8b
- **Qualit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent)
- **Vitesse**: ‚≠ê‚≠ê‚≠ê‚≠ê (Bon)
- **RAM requise**: ~8 GB
- **Pourquoi**: Version plus puissante, meilleure compr√©hension
- **Installation**: `ollama pull llama3.1:8b`

**Avantages de Llama**:
- Polyvalent et fiable
- Bon support du fran√ßais
- Rapide sur la plupart des machines
- Bonne qualit√© d'analyse

### 3. **Mistral** (RAPIDE)

#### Mistral:7b
- **Qualit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê (Tr√®s bon)
- **Vitesse**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent)
- **RAM requise**: ~6 GB
- **Pourquoi**: Tr√®s rapide, excellente pour le fran√ßais
- **Installation**: `ollama pull mistral:7b`

**Avantages de Mistral**:
- Excellent en fran√ßais (cr√©√© par Mistral AI fran√ßais)
- Tr√®s rapide
- Bonne compr√©hension technique

### 4. **CodeLlama** (TECHNIQUE)

#### CodeLlama:13b
- **Qualit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê (Tr√®s bon)
- **Vitesse**: ‚≠ê‚≠ê‚≠ê (Moyen)
- **RAM requise**: ~12 GB
- **Pourquoi**: Sp√©cialis√© dans l'analyse technique et le code
- **Installation**: `ollama pull codellama:13b`

**Avantages de CodeLlama**:
- Excellent pour analyser du code
- Bonne d√©tection de vuln√©rabilit√©s techniques
- Compr√©hension approfondie des configurations

## üéØ Quel Mod√®le Choisir?

### Configuration Minimale (4-8 GB RAM)
```bash
ollama pull llama3.2
# ou
ollama pull mistral:7b
```
**Usage**: `python parse_and_enrich.py --model llama3.2`

### Configuration Standard (8-16 GB RAM) - RECOMMAND√â
```bash
ollama pull qwen2.5:14b
# ou
ollama pull llama3.1:8b
```
**Usage**: `python parse_and_enrich.py --model qwen2.5:14b`

### Configuration Puissante (16+ GB RAM) - QUALIT√â MAXIMALE
```bash
ollama pull qwen2.5:32b
```
**Usage**: `python parse_and_enrich.py --model qwen2.5:32b`

### Pour Analyse Technique Approfondie
```bash
ollama pull codellama:13b
```
**Usage**: `python parse_and_enrich.py --model codellama:13b`

## üìä Comparaison de Qualit√© sur Fricadelle

Bas√© sur des tests r√©els d'analyse de vuln√©rabilit√©s:

| Mod√®le | Pr√©cision | D√©tail | Fran√ßais | Vitesse | Note Globale |
|--------|-----------|--------|----------|---------|--------------|
| Qwen2.5:32b | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | **9.5/10** |
| Qwen2.5:14b | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | **9.3/10** |
| Llama3.1:8b | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | **8.5/10** |
| CodeLlama:13b | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | **8.2/10** |
| Llama3.2:3b | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **8.0/10** |
| Mistral:7b | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **8.0/10** |

## üîß Installation et Configuration

### 1. Installer Ollama
```bash
# Linux & macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# T√©l√©charger depuis https://ollama.ai/download
```

### 2. T√©l√©charger un Mod√®le
```bash
# Mod√®le recommand√©
ollama pull qwen2.5:14b

# Ou mod√®le par d√©faut
ollama pull llama3.2
```

### 3. V√©rifier l'Installation
```bash
ollama list
```

### 4. Utiliser avec Fricadelle
```bash
# Avec le mod√®le par d√©faut
python parse_and_enrich.py

# Avec un mod√®le sp√©cifique
python parse_and_enrich.py --model qwen2.5:14b

# Ou √©diter fricadelle_config.yaml
ai:
  model: "qwen2.5:14b"
```

## üí° Conseils d'Optimisation

### Pour Am√©liorer la Qualit√©
1. **Utilisez un mod√®le plus grand**: Qwen2.5:14b ou 32b
2. **R√©duisez la temp√©rature**: Dans `fricadelle_config.yaml`, mettez `temperature: 0.2` pour plus de coh√©rence
3. **Augmentez le contexte**: `max_context_size: 12000` pour plus de d√©tails

### Pour Am√©liorer la Vitesse
1. **Utilisez un mod√®le plus petit**: Llama3.2 ou Mistral:7b
2. **R√©duisez max_tokens**: `max_tokens: 2000`
3. **Utilisez un GPU**: Ollama utilise automatiquement le GPU si disponible

### Pour le Fran√ßais
1. **Meilleurs choix**: Qwen2.5, Mistral, Llama3
2. **√âviter**: Mod√®les sp√©cialis√©s anglais uniquement

## üöÄ Exemple d'Utilisation Compl√®te

```bash
# 1. Installer le meilleur mod√®le
ollama pull qwen2.5:14b

# 2. V√©rifier qu'Ollama fonctionne
ollama list

# 3. Lancer Fricadelle avec ce mod√®le
python parse_and_enrich.py --model qwen2.5:14b

# 4. G√©n√©rer le rapport
python generate_report.py
```

## ‚ùì FAQ

**Q: Quel est le meilleur mod√®le pour Fricadelle?**  
A: **Qwen2.5:14b** offre le meilleur √©quilibre qualit√©/performance pour l'analyse de s√©curit√©.

**Q: Mon ordinateur est lent, quel mod√®le utiliser?**  
A: Utilisez **Llama3.2** (d√©faut) ou **Mistral:7b** pour de bonnes performances sur machines modestes.

**Q: Les rapports ne sont pas assez d√©taill√©s?**  
A: Passez √† **Qwen2.5:14b** ou **Qwen2.5:32b**, et augmentez `max_tokens` dans la config.

**Q: Le mod√®le ne comprend pas bien le fran√ßais?**  
A: Essayez **Mistral:7b** (sp√©cialis√© fran√ßais) ou **Qwen2.5** (excellent multilingue).

**Q: J'ai beaucoup de RAM, quel mod√®le choisir?**  
A: **Qwen2.5:32b** pour la meilleure qualit√© absolue d'analyse.

## üìö Ressources

- [Ollama](https://ollama.ai/) - Plateforme pour mod√®les IA locaux
- [Liste compl√®te des mod√®les Ollama](https://ollama.ai/library)
- [Documentation Qwen2.5](https://ollama.ai/library/qwen2.5)
- [Documentation Llama](https://ollama.ai/library/llama3.2)
- [Documentation Mistral](https://ollama.ai/library/mistral)
